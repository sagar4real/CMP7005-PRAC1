{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7F6TB7uH5v6zuw4Na/0bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar4real/CMP7005-PRAC1/blob/main/01_data_handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sagar4real/CMP7005-PRAC1.git"
      ],
      "metadata": {
        "id": "C83KKFouQikk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1 - DATA HANDLING**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yvxrY7_q31gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Import Libraries**"
      ],
      "metadata": {
        "id": "lywScCei4BxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh2dlGVzDXcM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Load and Inspect Individual CSV Files**"
      ],
      "metadata": {
        "id": "GtMHqQsV4Ve1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder where your CSVs are stored\n",
        "DATA_FOLDER = \"/content/CMP7005-PRAC1/data\"\n",
        "\n",
        "# Pattern to match your files\n",
        "pattern = os.path.join(DATA_FOLDER, \"*_data.csv\")\n",
        "\n",
        "# List all matching files\n",
        "csv_files = sorted(glob.glob(pattern))\n",
        "\n",
        "for(file) in csv_files:\n",
        "    temp = pd.read_csv(file)\n",
        "    print(file.split('/')[4] , \" = \" , temp.shape)"
      ],
      "metadata": {
        "id": "TR83-ciT4t6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Merge All _data.csv Files**"
      ],
      "metadata": {
        "id": "pQ656C9_5t-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(csv_files) == 0:\n",
        "    raise FileNotFoundError(\"No files found !\")\n",
        "\n",
        "# Load and append\n",
        "df_list = []\n",
        "for file in csv_files:\n",
        "    temp = pd.read_csv(file)\n",
        "    df_list.append(temp)\n",
        "\n",
        "# Merge all into one DataFrame\n",
        "merged_df = pd.concat(df_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "Mwkk_Jahw_BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.5 Basic Inspection of the Merged Dataset**"
      ],
      "metadata": {
        "id": "sy5NBTWF58JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preview top rows\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "-v1duofuzb6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview buttom rows\n",
        "merged_df.tail()"
      ],
      "metadata": {
        "id": "z7lsJeUllfCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataset structure\n",
        "merged_df.info()"
      ],
      "metadata": {
        "id": "eVmb4x1Fzda0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values\n",
        "merged_df.isnull().sum().sort_values(ascending=False).head(20)"
      ],
      "metadata": {
        "id": "ht5Y43kyzzM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "merged_df.describe(include='all')\n"
      ],
      "metadata": {
        "id": "LbxVLrX3z3bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Report**\n",
        "\n",
        "In this exercise I have downloaded the India Air Quality data set from Moodle and there were several csv files named ending in _data.csv. I wanted to use the same data for my analysis therefore I loaded each file into Jupyter Notebook and did an initial evaluation of the data.\n",
        "\n",
        "First I made a list of all csv files (with the names of the csv files) available in the dataset folder. Then I loaded each one of these files separately. I then concatenated the individual files together into a single data set using **pandas.concat()** with ignore_index=True to prevent duplicate indexes.\n",
        "\n",
        "Once the data had been merged, I conducted a preliminary assessment of the data using **.head()**, **.info()**, **.isnull().sum()** and **.describe()** to assess the structure of the data, the type of data in each column and how many missing values are present. The merged data set is now stored as **merged_df.csv** and will be the source data for the EDA in Task 2."
      ],
      "metadata": {
        "id": "xk8h9gg37rhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2 - Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "_PZU0RxnR904"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Fundamental data understanding**"
      ],
      "metadata": {
        "id": "PHACJAH6TD1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.1 Basic shape, columns, dtypes, sample values**"
      ],
      "metadata": {
        "id": "YsQEiOAzTYZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Shape, columns and dtypes\n",
        "print(\"Rows, Columns:\", merged_df.shape)\n",
        "print(\"\\nColumns:\")\n",
        "print(merged_df.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "display(merged_df.dtypes)\n"
      ],
      "metadata": {
        "id": "6UvCckGBTv9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.2 Count of missing values & percent missing**\n"
      ],
      "metadata": {
        "id": "Y44Gj0SMUyJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Missing values summary\n",
        "missing = merged_df.isnull().sum().sort_values(ascending=False)\n",
        "missing_pct = (missing / len(merged_df) * 100).round(2)\n",
        "pd.concat([missing, missing_pct.rename(\"pct\")], axis=1).head(30)\n"
      ],
      "metadata": {
        "id": "sPzu_2iydoWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1.3 Unique values for categorical fields & sample city counts**"
      ],
      "metadata": {
        "id": "P00isr0ceDzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 Unique categorical summaries\n",
        "for col in ['City','AQI_Bucket','aqi_bucket','City_Name']:\n",
        "    if col in merged_df.columns:\n",
        "        print(f\"\\nValue counts for {col}:\")\n",
        "        display(merged_df[col].value_counts().head(10))\n",
        "\n",
        "# generic approach: list object columns and show top values\n",
        "obj_cols = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(\"\\nObject columns detected:\", obj_cols)\n",
        "for c in obj_cols:\n",
        "    print(f\"\\nTop values for '{c}':\")\n",
        "    display(merged_df[c].value_counts().head(8))\n"
      ],
      "metadata": {
        "id": "vbO45jvHeOUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Data Preprocessing (cleaning, missing handling, duplicates, feature engineering)**"
      ],
      "metadata": {
        "id": "z8tvGQcvfuDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1 Remove exact duplicate rows(if any)**"
      ],
      "metadata": {
        "id": "F259C_cHgayz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_duplicates(df, title=\"Duplicate Check\", preview_rows=5):\n",
        "    \"\"\"\n",
        "    Visualizes duplicate information in a DataFrame.\n",
        "    Shows:\n",
        "      - Count of duplicate rows\n",
        "      - Percentage of duplicates\n",
        "      - Preview of duplicated rows\n",
        "      - Bar chart visualizing duplicates vs uniques\n",
        "    \"\"\"\n",
        "    dup_mask = df.duplicated(keep=False)\n",
        "    dup_df = df[dup_mask]\n",
        "\n",
        "    dup_count = dup_df.shape[0]\n",
        "    total = df.shape[0]\n",
        "    unique_count = total - dup_count\n",
        "    percent_dup = round((dup_count / total) * 100, 2)\n",
        "    percent_unique = round((unique_count / total) * 100, 2)\n",
        "\n",
        "\n",
        "    # ----- BAR CHART -----\n",
        "    categories = ['Duplicates', 'Unique']\n",
        "    counts = [dup_count, unique_count]\n",
        "    percentages = [percent_dup, percent_unique]\n",
        "    colors = ['red', 'green']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    bars = ax.bar(categories, counts, color=colors, edgecolor='black', alpha=0.8)\n",
        "\n",
        "    # Add count and percentage labels on top of bars\n",
        "    for i, (bar, count, perc) in enumerate(zip(bars, counts, percentages)):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
        "                f'{count} ({perc}%)',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Add total count annotation\n",
        "    # ax.text(0.5, 1.02, f'Total Rows: {total}', transform=ax.transAxes,\n",
        "    #         ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "    ax.set_ylabel('Number of Rows', fontweight='bold')\n",
        "    ax.set_title(f'{title}\\nDuplicate vs Unique Rows', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Set y-axis limit with some padding\n",
        "    ax.set_ylim(0, max(counts) * 1.15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"DUPLICATE ANALYSIS SUMMARY:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total rows: {total}\")\n",
        "    print(f\"Duplicate rows: {dup_count} ({percent_dup}%)\")\n",
        "    print(f\"Unique rows: {unique_count} ({percent_unique}%)\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "EPkUtwoLgkEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_duplicates(merged_df)"
      ],
      "metadata": {
        "id": "TkPO9j5GhrsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon performing a duplicate row analysis, no duplicate entries were identified in the dataset. Therefore, no duplicate removal was required at this stage."
      ],
      "metadata": {
        "id": "lIlez5F_Wyz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2 Convert Date to datetime & Extract Month**"
      ],
      "metadata": {
        "id": "BvKXv_txd6vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date column to datetime (DD/MM/YYYY format)\n",
        "merged_df['Date'] = pd.to_datetime(merged_df['Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
        "\n",
        "# Extract the Month for imputation\n",
        "merged_df['Month'] = merged_df['Date'].dt.month\n",
        "\n",
        "merged_df[['Date', 'Month']].head()"
      ],
      "metadata": {
        "id": "1OykTtpkeD2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert the Date column into a proper datetime format and extract the Month so we can group the data by City and Month during missing value imputation."
      ],
      "metadata": {
        "id": "Wmd6C66Berxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3 Handeling missing values**"
      ],
      "metadata": {
        "id": "D3XF-_VpXtG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_missing(df, title=\"Missing Value Analysis\"):\n",
        "    \"\"\"\n",
        "    Visualizes missing values in a DataFrame.\n",
        "    Shows:\n",
        "      - Missing count\n",
        "      - Missing percentage\n",
        "      - Bar chart of missing percentage\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate missing data\n",
        "    missing_count = df.isna().sum()\n",
        "    missing_percent = (df.isna().sum() / len(df)) * 100\n",
        "\n",
        "    # Combine into a DataFrame for display\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_count,\n",
        "        'Missing %': missing_percent.round(2)\n",
        "    }).sort_values(by='Missing %', ascending=False)\n",
        "\n",
        "    print(f\"\\n===== {title} =====\")\n",
        "    display(missing_df.head(80))   # show top 20 columns with missing values\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    missing_df['Missing %'].plot(kind='bar', color='salmon')\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Percentage Missing (%)\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "35Hd33NgZozS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_missing(merged_df, title=\"Before Handling Missing Values\")\n"
      ],
      "metadata": {
        "id": "1LGfyrN9erJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Missing Value Handling Method (Explanation)\n",
        "\n",
        "To handle missing values in the India Air Quality dataset, we use a **three-level median-based imputation strategy**, which is the most suitable and scientifically accurate approach for this type of data.\n",
        "\n",
        "###  1. City + Month Median (Main Imputation)\n",
        "Air pollution levels vary significantly between different **cities** and different **months/seasons**.  \n",
        "Using the *median value for each Cityâ€“Month* group ensures that:\n",
        "\n",
        "- Seasonal patterns are preserved  \n",
        "- City-specific pollution behaviour is maintained  \n",
        "- Imputation remains realistic and context-aware  \n",
        "\n",
        "This prevents mixing values from cities with very different pollution levels.\n",
        "\n",
        "---\n",
        "\n",
        "###  2. City Median (Backup Imputation)\n",
        "Some pollutants (e.g., Xylene, Benzene, Toluene, NH3) have long periods where data is not recorded in certain months.\n",
        "\n",
        "If the Cityâ€“Month median cannot be computed, we fill missing values using the **City median**.  \n",
        "This keeps the regional pollution characteristics consistent without relying on unrelated cities.\n",
        "\n",
        "---\n",
        "\n",
        "###  3. Global Median (Final Fallback)\n",
        "If values remain missing after the first two steps, we use the **overall dataset median**.  \n",
        "This guarantees:\n",
        "- No missing values remain  \n",
        "- Imputation stays robust  \n",
        "- Outliers do not distort the results (median is resistant to extreme values)\n",
        "\n",
        "---\n",
        "\n",
        "##  Why This Method Is the Best for This Dataset\n",
        "This dataset has:\n",
        "- Very high missingness in VOCs (up to 61%)  \n",
        "- Large differences between cities  \n",
        "- Strong seasonal pollution variation  \n",
        "\n",
        "A simple mean/median imputation would distort pollution trends.\n",
        "\n",
        "This **three-level hierarchical method** is ideal because it:\n",
        "\n",
        "âœ” Preserves real-world pollution patterns  \n",
        "âœ” Respects city-level differences  \n",
        "âœ” Handles large missingness sensibly  \n",
        "âœ” Avoids introducing bias  \n",
        "âœ” Produces scientifically reliable values  \n",
        "\n",
        "This prepares the dataset for accurate EDA, modelling, and further analysis.\n"
      ],
      "metadata": {
        "id": "G7E-LWq-htmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3.1 Select Numeric Pollutant Columns**"
      ],
      "metadata": {
        "id": "QUxSZOPineao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of pollutant + AQI columns we want to impute\n",
        "pollutant_cols = [\n",
        "    'PM2.5','PM10','NO','NO2','NOx','NH3',\n",
        "    'CO','SO2','O3','Benzene','Toluene','Xylene','AQI'\n",
        "]\n",
        "\n",
        "# Keep only the ones that actually exist in your dataframe\n",
        "pollutant_cols = [col for col in pollutant_cols if col in merged_df.columns]\n",
        "\n",
        "pollutant_cols\n"
      ],
      "metadata": {
        "id": "CcuJITz4f5vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We detect the numeric pollutant columns to ensure that only real pollutant values (PM2.5, NO2, SO2, etc.) are cleaned, preventing accidental imputation of columns like City or Date."
      ],
      "metadata": {
        "id": "nEbTBrGdgRRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3.2 Create the Imputation Function**"
      ],
      "metadata": {
        "id": "JFXAApBQpWqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_pollutants(df, pollutant_cols):\n",
        "    \"\"\"\n",
        "    Imputes missing values for pollutant columns using:\n",
        "    1. City + Month median\n",
        "    2. City median\n",
        "    3. Global median\n",
        "    \"\"\"\n",
        "    for col in pollutant_cols:\n",
        "\n",
        "        # 1. Fill using City + Month median\n",
        "        df[col] = df.groupby(['City', 'Month'])[col].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # 2. If still missing, fill with City median\n",
        "        df[col] = df.groupby('City')[col].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # 3. Final fallback: global median\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "4s6n-I3MptPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing pollutant values were imputed using a three-level hierarchical median strategy: first at the Cityâ€“Month level, then at the City level, and finally using the global median as a fallback. This method was chosen because air pollutant concentrations strongly depend on both geographical location and seasonal variation, and this approach preserves these real-world spatial and temporal patterns while ensuring no missing values remain."
      ],
      "metadata": {
        "id": "l8fsTaMhtdpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_mearged_df = impute_pollutants(merged_df, pollutant_cols)"
      ],
      "metadata": {
        "id": "8H_qec9NtlaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.3.3 Extract unique values and counts for AQI_Bucket**"
      ],
      "metadata": {
        "id": "caQOyldPv9fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since **AQI_Bucket** is a categorical variable derived from the numeric AQI values, we first analyse all unique category labels to check for inconsistencies and missing values. We then standardise the categories and reconstruct missing labels using official AQI threshold rules to ensure logical consistency between AQI and AQI_Bucket before further analysis."
      ],
      "metadata": {
        "id": "fJc-Ut6PwREt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_mearged_df['AQI_Bucket'] = cleaned_mearged_df['AQI_Bucket'].astype(str).str.strip()\n",
        "\n",
        "print(\"Unique AQI_Bucket values:\")\n",
        "print(cleaned_mearged_df['AQI_Bucket'].unique())\n",
        "\n",
        "print(\"\\nValue counts (including missing as 'nan'):\")\n",
        "cleaned_mearged_df['AQI_Bucket'].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "adZQo7BSwQkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_aqi_bucket(df, aqi_col='AQI', bucket_col='AQI_Bucket'):\n",
        "    \"\"\"\n",
        "    Cleans and fills AQI_Bucket using numeric AQI values.\n",
        "    Steps:\n",
        "    1. Standardises text values\n",
        "    2. Reconstructs missing categories from AQI using official thresholds\n",
        "    3. Labels remaining missing values as 'Unknown'\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Standardise AQI_Bucket values ---\n",
        "    df[bucket_col] = df[bucket_col].astype(str).str.strip()\n",
        "    df[bucket_col] = df[bucket_col].replace(['nan', 'None', 'NaN', ''], np.nan)\n",
        "\n",
        "    # --- AQI classification rules (India standard) ---\n",
        "    def classify_aqi_bucket(aqi):\n",
        "        if pd.isna(aqi):\n",
        "            return np.nan\n",
        "        elif aqi <= 50:\n",
        "            return \"Good\"\n",
        "        elif aqi <= 100:\n",
        "            return \"Satisfactory\"\n",
        "        elif aqi <= 200:\n",
        "            return \"Moderate\"\n",
        "        elif aqi <= 300:\n",
        "            return \"Poor\"\n",
        "        elif aqi <= 400:\n",
        "            return \"Very Poor\"\n",
        "        else:\n",
        "            return \"Severe\"\n",
        "\n",
        "    # --- Fill missing AQI_Bucket using AQI ---\n",
        "    df[bucket_col] = df[bucket_col].fillna(\n",
        "        df[aqi_col].apply(classify_aqi_bucket)\n",
        "    )\n",
        "\n",
        "    # --- Final fallback ---\n",
        "    df[bucket_col] = df[bucket_col].fillna(\"Unknown\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "8TlzqY0UyYKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function standardises the AQI_Bucket column, reconstructs missing category values using the corresponding numeric AQI based on official threshold rules, and assigns â€˜Unknownâ€™ where both values are unavailable.â€"
      ],
      "metadata": {
        "id": "GLiv9YTyzLcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "removed_missing_value_df = clean_aqi_bucket(cleaned_mearged_df)"
      ],
      "metadata": {
        "id": "VusCw0SMykM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_missing(removed_missing_value_df, title=\"After Handling Missing Values\")"
      ],
      "metadata": {
        "id": "ZC5z80v7tpsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.4 Outlier Detection & Treatment**"
      ],
      "metadata": {
        "id": "rvr5Xi0LFJ2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we analyse and treat outliers in key pollutant variables such as PM2.5, PM10, NOâ‚‚, SOâ‚‚, CO, and Oâ‚ƒ. Outliers are identified using visual methods (boxplots) and statistical boundaries (IQR/percentile limits), then capped using winsorization to prevent extreme pollution spikes from distorting the analysis and future predictive models."
      ],
      "metadata": {
        "id": "-75G0k-wGvy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.4.1 Select Key Pollutant Columns for Outlier Analysis**"
      ],
      "metadata": {
        "id": "pCJB_t2hMpHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier detection will be applied only to the primary continuous pollutants (PM2.5, PM10, NOâ‚‚, CO, SOâ‚‚, and Oâ‚ƒ) because these variables exhibit high variability, wide value ranges, and a strong influence on air quality. Categorical variables and sparse VOC measurements were excluded as outlier analysis is not statistically meaningful for them at this stage."
      ],
      "metadata": {
        "id": "sf-uFwRuNCgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core pollutant columns for outlier analysis\n",
        "outlier_cols = ['PM2.5', 'PM10', 'NO2', 'CO', 'SO2', 'O3']\n",
        "\n",
        "# Keep only columns that actually exist in your dataset\n",
        "outlier_cols = [col for col in outlier_cols if col in removed_missing_value_df.columns]\n",
        "\n",
        "outlier_cols\n"
      ],
      "metadata": {
        "id": "FqER_ssFM3Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_outliers_grid(df, cols, title_prefix=\"Outlier Visualization\"):\n",
        "    \"\"\"\n",
        "    Visualizes outliers using boxplots arranged 2 per row.\n",
        "    \"\"\"\n",
        "    num_cols = len(cols)\n",
        "    rows = math.ceil(num_cols / 2)\n",
        "\n",
        "    plt.figure(figsize=(12, 4 * rows))\n",
        "\n",
        "    for i, col in enumerate(cols, 1):\n",
        "        plt.subplot(rows, 2, i)\n",
        "        sns.boxplot(x=df[col], color='skyblue')\n",
        "        plt.title(f\"{title_prefix}: {col}\")\n",
        "        plt.xlabel(col)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ppIJbXCGURYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use boxplots to visualise outliers because they effectively show the distribution of pollutant values, identify extreme points beyond the whiskers, and allow quick comparison before and after outlier treatment."
      ],
      "metadata": {
        "id": "gBRGorLxJ9LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_outliers_grid(removed_missing_value_df, outlier_cols, title_prefix=\"Before Outlier Treatment\")"
      ],
      "metadata": {
        "id": "zmd4dOqPKGA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.4.2 IQR Outlier Capping Function for Outlier Treatment**"
      ],
      "metadata": {
        "id": "8jI0YfSTNUPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Interquartile Range (IQR) method identifies outliers based on the spread of the middle 50% of the data. Values falling below\n",
        "ð‘„\n",
        "1\n",
        "âˆ’\n",
        "1.5\n",
        "Ã—\n",
        "ð¼\n",
        "ð‘„\n",
        "ð‘…\n",
        "Q1âˆ’1.5Ã—IQR or above\n",
        "ð‘„\n",
        "3\n",
        "+\n",
        "1.5\n",
        "Ã—\n",
        "ð¼\n",
        "ð‘„\n",
        "ð‘…\n",
        "Q3+1.5Ã—IQR are considered extreme. Instead of removing these values, IQR-based capping limits them to the acceptable boundary range, reducing the influence of extreme pollution spikes while preserving all observations."
      ],
      "metadata": {
        "id": "GU8sr9o4Npkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cap_outliers_iqr(df, cols):\n",
        "    \"\"\"\n",
        "    Caps outliers using the IQR method for the given columns.\n",
        "    Outliers below the lower bound or above the upper bound are clipped.\n",
        "    \"\"\"\n",
        "    for col in cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "tuRjxRWgN2pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers were treated using the Interquartile Range (IQR) method, where extreme values were capped at the lower and upper statistical bounds instead of being removed."
      ],
      "metadata": {
        "id": "IFmU6EW7N7Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_draft_df = cap_outliers_iqr(removed_missing_value_df, outlier_cols)\n"
      ],
      "metadata": {
        "id": "eoZAqLSCOB5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_outliers_grid(final_draft_df, outlier_cols, title_prefix=\"After Outlier Treatment\")"
      ],
      "metadata": {
        "id": "oLfMzCANOMzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_missing_value_df.head(20)"
      ],
      "metadata": {
        "id": "ct-IJBqFmM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Statistcs/computation-based analysis and Visualisation**"
      ],
      "metadata": {
        "id": "jMAu8Gc4Qn9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.1 Statistical Summary(Automatic Numeric Detection)**"
      ],
      "metadata": {
        "id": "_TN_67OHRcST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = final_draft_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Generate statistical summary\n",
        "final_draft_df[numeric_cols].describe().T\n"
      ],
      "metadata": {
        "id": "o_5zwX_UmOxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An  statistical summary was computed for all numeric variables to quantify central tendency, dispersion, and range using metrics such as mean, standard deviation, quartiles, and extrema. This numerical profiling provides a baseline understanding of pollutant concentration distributions prior to visual exploratory analysis."
      ],
      "metadata": {
        "id": "nCTLqTMMSOMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.2 Univariate Analysis**"
      ],
      "metadata": {
        "id": "aiOmh7XZSpgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will perform univariate analysis to understand the distribution and behaviour of individual air-quality variables. Histograms are used to visualise numeric pollutant concentrations because they effectively show how values are spread, whether the data is skewed, and which ranges occur most frequently. A bar chart is used for the categorical variable AQI_Bucket to display the frequency of each air-quality category. These visualisations help identify dominant pollution levels and provide a foundation for deeper analysis in later steps."
      ],
      "metadata": {
        "id": "IQMP7tvRUuco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = final_draft_df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "exclude_cols = ['Month', 'Year', 'Day']\n",
        "numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "# Limit number of plots if you want (e.g., first 6 numeric columns)\n",
        "plot_cols = numeric_cols[:6]\n",
        "\n",
        "# --- Dynamic grid size: 2 columns ---\n",
        "n_cols = 2\n",
        "n_rows = math.ceil(len(plot_cols) / n_cols)\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# --- Loop through columns and plot histograms ---\n",
        "for i, col in enumerate(plot_cols):\n",
        "    axes[i].hist(final_draft_df[col].dropna(), bins=30)\n",
        "    axes[i].set_title(f\"Distribution of {col}\")\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel(\"Frequency\")\n",
        "\n",
        "# --- Turn off any unused subplots ---\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ic-U4RatSvfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "final_draft_df['AQI_Bucket'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of AQI Categories\")\n",
        "plt.xlabel(\"AQI Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hm-kpnlsUdWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histograms show that most pollutants in the dataset are right-skewed, with many low to moderate values and fewer high-concentration days. PM2.5 and PM10 exhibit broad distributions with noticeable peaks, reflecting frequent moderate pollution events. Gaseous pollutants such as NO, NOâ‚‚, NOx and NHâ‚ƒ show heavy right skew, indicating that high concentrations occur occasionally but not regularly. The AQI category distribution shows that â€˜Moderateâ€™ and â€˜Satisfactoryâ€™ air-quality levels dominate the dataset, while â€˜Poorâ€™, â€˜Very Poorâ€™, and â€˜Severeâ€™ categories occur less frequently. Overall, the univariate analysis highlights the variability of pollutant levels and confirms that most pollutants have rare but significant high-value spikes."
      ],
      "metadata": {
        "id": "lIIAVxwOP5Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.3 Bivariate Analysis**"
      ],
      "metadata": {
        "id": "dABYT-uJV6K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, bivariate analysis is performed to examine the relationships between pairs of pollutants and air-quality indicators. Scatter plots are used to visualise the strength and direction of relationships between variables, while a line plot is used to observe how pollution levels change over time. These visualisations help identify correlations and real-world dependencies between air-quality variables."
      ],
      "metadata": {
        "id": "PKbgIvIhWKbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define bivariate plot pairs dynamically (only if columns exist) ---\n",
        "bivariate_pairs = []\n",
        "\n",
        "if 'PM2.5' in final_draft_df.columns and 'AQI' in final_draft_df.columns:\n",
        "    bivariate_pairs.append(('PM2.5', 'AQI', 'scatter'))\n",
        "\n",
        "if 'NO2' in final_draft_df.columns and 'O3' in final_draft_df.columns:\n",
        "    bivariate_pairs.append(('NO2', 'O3', 'scatter'))\n",
        "\n",
        "if 'Date' in final_draft_df.columns and 'PM2.5' in final_draft_df.columns:\n",
        "    bivariate_pairs.append(('Date', 'PM2.5', 'line'))\n",
        "\n",
        "# --- Grid layout (2 plots per row) ---\n",
        "n_cols = 2\n",
        "n_rows = math.ceil(len(bivariate_pairs) / n_cols)\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# --- Plot dynamically ---\n",
        "for i, (x, y, plot_type) in enumerate(bivariate_pairs):\n",
        "\n",
        "    if plot_type == 'scatter':\n",
        "        axes[i].scatter(final_draft_df[x], final_draft_df[y], alpha=0.5)\n",
        "        axes[i].set_title(f\"{x} vs {y}\")\n",
        "        axes[i].set_xlabel(x)\n",
        "        axes[i].set_ylabel(y)\n",
        "\n",
        "    elif plot_type == 'line':\n",
        "        axes[i].plot(final_draft_df[x], final_draft_df[y])\n",
        "        axes[i].set_title(f\"{y} Over Time\")\n",
        "        axes[i].set_xlabel(\"Date\")\n",
        "        axes[i].set_ylabel(y)\n",
        "\n",
        "# --- Hide unused subplots ---\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0bQdGgSBWhwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot of PM2.5 vs AQI shows a clear positive relationship, indicating that increases in PM2.5 levels strongly contribute to higher AQI values. The NO2 vs O3 scatter shows a weak, scattered relationship, suggesting these pollutants do not vary together consistently and may be influenced by different emission or atmospheric processes. The PM2.5 time-series plot shows strong fluctuations across years, reflecting seasonal variation with higher pollution during winter months and lower levels during monsoon seasons."
      ],
      "metadata": {
        "id": "m9eY8gwLOtxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.4 Multivariate Analysis**"
      ],
      "metadata": {
        "id": "sGzYoYmCqXAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, multivariate analysis is performed to examine the relationships among multiple pollutants simultaneously. A correlation heatmap is used to visualise the strength and direction of relationships between variables, helping to identify strongly related pollutants and potential multicollinearity."
      ],
      "metadata": {
        "id": "pXDFdiAwqk22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = final_draft_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = numeric_cols.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Air Quality Variables\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l1vO8FRZqnrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap shows that PM2.5 and PM10 are strongly correlated (0.69), indicating that fine and coarse particulate matter increase together. AQI is also strongly related to PM2.5 (0.68) and PM10 (0.50), confirming that particulate matter is the main driver of air-quality degradation. NO and NOx show a very strong correlation (0.76), as expected due to their chemical relationship. Other pollutants such as NH3, SO2, O3, and VOCs show weak correlations, indicating they vary more independently.\n"
      ],
      "metadata": {
        "id": "34IWSi5Krszi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Report**\n",
        "\n",
        "The dataset was first explored to understand its structure, data types, and missing-value patterns. Several pollutant variables such as PM2.5, PM10, NH3, and VOCs contained substantial missing data, while City and Date were complete. Duplicate checking confirmed that no repeated records were present. A hierarchical median-based imputation strategy (Cityâ€“Month â†’ City â†’ Global median) was applied to handle missing numeric values. The categorical variable AQI_Bucket was reconstructed from numeric AQI values using official threshold rules, and any remaining missing labels were assigned as â€œUnknown.â€\n",
        "\n",
        "Outlier detection was performed on key continuous pollutants (PM2.5, PM10, NO2, CO, SO2, and O3) using boxplots. Extreme values were treated using the Interquartile Range (IQR) capping method to reduce their influence while preserving all observations. Statistical summaries were generated automatically for all numeric features to understand the central tendency, spread, and range of pollutants. Univariate visualisations, including histograms and bar charts, were used to examine individual pollutant distributions and AQI category frequencies.\n",
        "\n",
        "Bivariate and multivariate analyses were then conducted to explore relationships between variables. Scatter plots showed strong relationships between PM2.5 and AQI, and between nitrogen-based pollutants. A correlation heatmap revealed that PM2.5 and PM10 have the strongest influence on AQI, while several gaseous pollutants and VOCs exhibited weak correlations. Overall, the EDA highlighted key temporal, spatial, and pollutant-level patterns and prepared the dataset for predictive modelling in the next task."
      ],
      "metadata": {
        "id": "dlAIN2CBsJ7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 3 - Model Building**"
      ],
      "metadata": {
        "id": "y-1Nx9k1fWcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Task 3, we build predictive models to estimate air-quality levels based on pollutant data. This includes selecting an appropriate target variable, preparing machine-learning features, splitting data into training and testing sets, developing baseline and advanced models, and evaluating their performance using metrics such as MAE, RMSE, and RÂ². The goal is to compare models and determine which provides the most accurate and reliable AQI prediction."
      ],
      "metadata": {
        "id": "FgAZJqHwfmup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Selecting the Target Variable AQI**\n",
        "Reason:\n",
        "\n",
        "\n",
        "\n",
        "*   AQI is the most meaningful public-facing air-quality indicator.\n",
        "*   It directly depends on pollutants.\n",
        "*   It's numerical â†’ easy to model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bj-JnC4QgGEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_draft_df['AQI'].isna().sum(), final_draft_df['AQI'].describe()"
      ],
      "metadata": {
        "id": "ykhA7LsqnZMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Create X and Y (Features + Target)**"
      ],
      "metadata": {
        "id": "9HZL2X_Cn3Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a working copy\n",
        "model_df = final_draft_df.copy()\n",
        "\n",
        "# Columns we should NOT use as numeric features\n",
        "drop_cols = ['Date', 'AQI_Bucket']\n",
        "\n",
        "model_df = model_df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# Convert City into numeric (one-hot encoding)\n",
        "model_df = pd.get_dummies(model_df, columns=['City'], drop_first=True)\n",
        "\n",
        "# Target\n",
        "y = model_df['AQI']\n",
        "\n",
        "# Features\n",
        "X = model_df.drop(columns=['AQI'])\n"
      ],
      "metadata": {
        "id": "p9mZD0GDoPBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AQI was selected as the target variable since it is complete and provides a meaningful numeric measure of air quality. Categorical variables such as City were converted into dummy variables, and non-predictive fields like Date and AQI_Bucket were removed to create a clean feature matrix for model development."
      ],
      "metadata": {
        "id": "hdWT6AlhoVoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Train-Test Split**"
      ],
      "metadata": {
        "id": "m5cXiHEnonuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Check shapes\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
      ],
      "metadata": {
        "id": "AOLRfZnpourb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was split into training (80%) and testing (20%) sets using a fixed random seed to ensure reproducible and unbiased evaluation of model performance.â€"
      ],
      "metadata": {
        "id": "tZ2gWUF3oyG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4 Train Baseline Model (Linear Regression)**"
      ],
      "metadata": {
        "id": "la8_1rA5o7aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is used as a baseline to provide a simple reference point for model performance."
      ],
      "metadata": {
        "id": "YB4_WPVBpDYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "lr_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "OSrxllfKpA4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.5 Evalute the Baseline Model**\n"
      ],
      "metadata": {
        "id": "SV5rz_RSqQZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate using:\n",
        "\n",
        "*   **MAE:** Average absolute error\n",
        "*   **RMSE:** Penalises larger errors\n",
        "*   **RÂ²:** Proportion of variance explained\n",
        "\n"
      ],
      "metadata": {
        "id": "FtHUS_BeqZNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "mae_lr, rmse_lr, r2_lr\n"
      ],
      "metadata": {
        "id": "p1sPsPvmqHUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.6 Train Random Forest Regressor**"
      ],
      "metadata": {
        "id": "Z-q7Q46mr-Mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest can capture non-linear relationships and interactions between pollutants, which Linear Regression cannot. This makes it well-suited for complex environmental data like air quality."
      ],
      "metadata": {
        "id": "vQUyDdjcsIOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "I8XAtWMcsLKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.7 Evaluate Random Forest Model**"
      ],
      "metadata": {
        "id": "j8Dc9G1Bs9qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "mae_rf, rmse_rf, r2_rf\n"
      ],
      "metadata": {
        "id": "QsI4WQrDt0gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Random Forest regression model was trained to capture non-linear relationships between pollutants and AQI. The model achieved lower error values and a higher RÂ² score compared to the baseline Linear Regression model, indicating improved predictive performance."
      ],
      "metadata": {
        "id": "CVF-Mlaysm3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.7 Comare Models**"
      ],
      "metadata": {
        "id": "xRTXxd87yEIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'Model': ['Linear Regression', 'Random Forest'],\n",
        "    'MAE': [mae_lr, mae_rf],\n",
        "    'RMSE': [rmse_lr, rmse_rf],\n",
        "    'R2 Score': [r2_lr, r2_rf]\n",
        "})\n",
        "\n",
        "results\n"
      ],
      "metadata": {
        "id": "7TPbYYkXyKZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model outperformed Linear Regression across all evaluation metrics, demonstrating its ability to model complex relationships in air-quality data more effectively.â€"
      ],
      "metadata": {
        "id": "CNxKV8o5yQQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.7.1 Bar Chart comparisons of Models**"
      ],
      "metadata": {
        "id": "qo2GWv2kyUsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "plt.bar(x - width/2, results.loc[0, metrics], width, label='Linear Regression')\n",
        "plt.bar(x + width/2, results.loc[1, metrics], width, label='Random Forest')\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "74vHdUC8ydjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart shows that the Random Forest model achieves lower MAE and RMSE and a higher RÂ² score compared to Linear Regression, indicating superior predictive accuracy and better generalisation to unseen data."
      ],
      "metadata": {
        "id": "QeJueI5lyj9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.7.2 Actual vs Predicted Scatter Plot**"
      ],
      "metadata": {
        "id": "tN6v2xsaymuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Linear Regression\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(y_test, y_pred_lr, alpha=0.4)\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()])\n",
        "plt.title('Linear Regression: Actual vs Predicted')\n",
        "plt.xlabel('Actual AQI')\n",
        "plt.ylabel('Predicted AQI')\n",
        "\n",
        "# Random Forest\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(y_test, y_pred_rf, alpha=0.4)\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()])\n",
        "plt.title('Random Forest: Actual vs Predicted')\n",
        "plt.xlabel('Actual AQI')\n",
        "plt.ylabel('Predicted AQI')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6gYU7KiFy30U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model shows predictions more closely aligned with the diagonal reference line, indicating better accuracy and reduced prediction error compared to Linear Regression.\n",
        "\n",
        "Both numeric metrics and visual comparisons confirm that the Random Forest model outperforms the Linear Regression baseline, making it the preferred model for AQI prediction in this study."
      ],
      "metadata": {
        "id": "jtt7N8tnzD_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 Report**\n",
        "\n",
        "In this task, predictive models were developed to estimate Air Quality Index (AQI) using the cleaned and preprocessed air-quality dataset. AQI was selected as the target variable because it provides a comprehensive numeric measure of air quality and contained no missing values. The dataset was prepared by removing non-predictive fields such as Date and AQI_Bucket, and categorical variables such as City were converted into numerical form using one-hot encoding. The data was then split into training (80%) and testing (20%) sets to enable unbiased evaluation of model performance.\n",
        "\n",
        "A baseline Linear Regression model was first trained to establish a reference level of performance. This model assumes a linear relationship between pollutant concentrations and AQI. Model performance was evaluated using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and the RÂ² score. To improve predictive accuracy, a Random Forest regression model was then developed. Random Forest was chosen due to its ability to capture non-linear relationships and complex interactions among pollutants. The Random Forest model achieved lower error values and a higher RÂ² score than the Linear Regression model, indicating superior predictive capability.\n",
        "\n",
        "Model comparison was further supported using visualisations, including bar charts of evaluation metrics and Actual vs Predicted scatter plots. These visual analyses showed that Random Forest predictions aligned more closely with actual AQI values, while Linear Regression exhibited greater dispersion and error. Overall, the Random Forest model outperformed the baseline model and was identified as the most suitable approach for AQI prediction in this study, demonstrating the effectiveness of ensemble methods for modelling complex environmental data."
      ],
      "metadata": {
        "id": "ErKhVLdz2KLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 4 - Application Development (Streamlit App)**"
      ],
      "metadata": {
        "id": "96rT0RLD_dtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task demonstrates the practical deployment of the developed AQI prediction model through an interactive application, allowing users to explore air-quality data and generate AQI predictions dynamically."
      ],
      "metadata": {
        "id": "g5y3vE4E_ox9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Save the Trained Random Forest Model**"
      ],
      "metadata": {
        "id": "xUMF1sEPAEBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Create models folder\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Save trained model\n",
        "joblib.dump(rf_model, \"/CMP7005-PRAC1/models/aqi_random_forest_model.pkl\")\n",
        "\n",
        "# Save feature names\n",
        "joblib.dump(X.columns.tolist(), \"/CMP7005-PRAC1/models/feature_names.pkl\")\n",
        "\n",
        "print(\"Model and feature names saved successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GsJeNtM4ADnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "QHmy5P2vbkrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# Load model and feature names\n",
        "model = joblib.load(\"models/aqi_random_forest_model.pkl\")\n",
        "feature_names = joblib.load(\"models/feature_names.pkl\")\n",
        "\n",
        "st.title(\"AQI Prediction Application\")\n",
        "st.write(\"Predict Air Quality Index using pollutant values\")\n",
        "\n",
        "# Sidebar inputs\n",
        "st.sidebar.header(\"Enter Pollutant Values\")\n",
        "\n",
        "PM25 = st.sidebar.number_input(\"PM2.5\", 0.0, 1000.0, 50.0)\n",
        "PM10 = st.sidebar.number_input(\"PM10\", 0.0, 1000.0, 100.0)\n",
        "NO2  = st.sidebar.number_input(\"NO2\", 0.0, 500.0, 40.0)\n",
        "CO   = st.sidebar.number_input(\"CO\", 0.0, 50.0, 1.0)\n",
        "SO2  = st.sidebar.number_input(\"SO2\", 0.0, 500.0, 10.0)\n",
        "O3   = st.sidebar.number_input(\"O3\", 0.0, 500.0, 30.0)\n",
        "\n",
        "# Create aligned input\n",
        "input_df = pd.DataFrame(0, index=[0], columns=feature_names)\n",
        "input_df['PM2.5'] = PM25\n",
        "input_df['PM10'] = PM10\n",
        "input_df['NO2'] = NO2\n",
        "input_df['CO'] = CO\n",
        "input_df['SO2'] = SO2\n",
        "input_df['O3'] = O3\n",
        "\n",
        "if st.button(\"Predict AQI\"):\n",
        "    prediction = model.predict(input_df)[0]\n",
        "    st.success(f\"Predicted AQI: {int(prediction)}\")\n"
      ],
      "metadata": {
        "id": "sC6qD6Lxbmc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}